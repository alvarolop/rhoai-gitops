= Red Hat OpenShift AI
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2024-03
// Metadata
:description: This repository is my playground to deploy, configure, and use RH OpenShift AI.
:keywords: openshift, red hat, machine learning, AI, RHOAI
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]
// End: Enable admonition icons

Red Hat OpenShift AI (RHOAI) builds on the capabilities of Red Hat OpenShift to provide a single, consistent, enterprise-ready hybrid AI and MLOps platform. It provides tools across the full lifecycle of AI/ML experiments and models including training, serving, monitoring, and managing AI/ML models and AI-enabled applications. This is my personal repository to test and play with some of its most important features.


== Red Hat Training

RHOAI is a product under continuous improvement, so *this repo will be outdated at some point in time*. I recommend you to refer to the https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest[Official documentation] to check the latest features or you can try the official trainings.

Red Hat OpenShift AI (RHOAI) is a platform for data scientists, AI practitioners, developers, machine learning engineers, and operations teams to prototype, build, deploy, and monitor AI models. This is a wide variety of audience that needs different kinds of training. For that reason, there are https://role.rhu.redhat.com/rol-rhu/app[several courses] that will help you to understand RHOAI from all angles:


* *AI262* - _Introduction to Red Hat OpenShift AI_: About configuring Data Science Projects and Jupyter Notebooks.
* *AI263* - _Red Hat OpenShift AI Administration_: About installing RHOAI, configuring users and permissions and creating Custom Notebook Images.
* *AI264* - _Creating Machine Learning Models with Red Hat OpenShift AI_: About training models and enhancing the model training.
* *AI265* - _Deploying Machine Learning Models with Red Hat OpenShift AI_: About serving models on RHOAI.
* *AI266* - _Automating AI/ML Workflows with Red Hat OpenShift AI_: About creating Data Science Pipelines, and Elyra and Kubeflow Pipelines.
* *AI267* - _Developing and Deploying AI/ML Applications on Red Hat OpenShift AI_: All the previous courses altogether.


== RHOAI Architecture

The following diagram depicts the general architecture of a RHOAI deployment, including the most important components:

.RHOAI Architecture
image::https://role.rhu.redhat.com/rol-rhu/static/static_file_cache/ai267-2.8/rhoaiarch/architecture/assets/architecture.svg[RHOAI Architecture]


* *codeflare*: Codeflare is an IBM software stack for developing and scaling machine-learning and Python workloads. It uses and needs the Ray component. The Codeflare component on RHOAI is a Technology Preview feature. If CodeFlare Operator has been installed in the cluster, it should be uninstalled first before enabled component.

* *dashboard*: Provides the RHOAI dashboard.

* *datasciencepipelines*: This enables you to build portable machine learning workflows. Requires the OpenShift Pipelines Operator to be present before enabling the data science pipelines.

* *kserve*: RHOAI uses Kserve to serve large language models that can scale based on demand. Requires the OpenShift Serverless and the OpenShift Service Mesh operators to be present before enabling the component. Does not support enabled ModelMeshServing at the same time.

* *kueue*: Kueue component configuration. It is not yet in Technology Preview

* *modelmeshserving*: KServe also offers a component for general-purpose model serving, called ModelMesh Mesh Serving. Activate this component to serve small and medium size models. Does not support enabled Kserve at the same time.

* *ray*: Component to run the data science code in a distributed manner. The Ray, or KubeRay, component on RHOAI is a Technology Preview feature.

* *workbenches*: Workbenches are containerized and isolated working environments for data scientists to examine data and work with data models. Data scientists can create workbenches from an existing notebook container image to access its resources and properties. Workbenches are associated to container storage to prevent data loss when the workbench container is restarted or deleted.





== Prerequisites

Due to the GitOps nature and that some files are confidential or change in every installation.

=== NVIDIA GPU nodes

First, you need to add an OCP node with GPUs. For that, you can use the AWS `g5.2xlarge` instance. According to the documentation:

> Amazon EC2 G5 instances are designed to accelerate graphics-intensive applications and machine learning inference. They can also be used to train simple to moderately complex machine learning models.

Use the following command if you are logged in to your cluster:

[source, bash]
----
oc process -f prerequisites/ocp-nodes/template-gpu-worker.yaml \
    -p INFRASTRUCTURE_ID=$(oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster) \
    -p INSTANCE_TYPE="g5.2xlarge" | \
    oc apply -n openshift-machine-api -f -
----

How to know that a node has NVIDIA GPUs using NodeFeatureDiscovery? The output of the following command will only be visible when you have applied the ArgoCD `Application` and the Node Feature Discovery operator has scanned the OpenShift nodes:

====
Run the following command and check the output:
[source, bash]
----
oc describe node | egrep 'Roles|pci'
Roles:              control-plane,master
Roles:              worker
                    feature.node.kubernetes.io/pci-1d0f.present=true
Roles:              gpu-worker,worker
                    feature.node.kubernetes.io/pci-10de.present=true
                    feature.node.kubernetes.io/pci-1d0f.present=true
Roles:              control-plane,master
Roles:              control-plane,master
----

`pci-10de` is the PCI vendor ID that is assigned to NVIDIA.
====

The NVIDIA GPU Operator uses the operator framework within Kubernetes to automate the management of all NVIDIA software components needed to provision GPU. These components include the NVIDIA drivers (to enable CUDA), Kubernetes device plugin for GPUs, the NVIDIA Container Runtime, automatic node labelling, DCGM based monitoring and others.

After configuring the Node Feature Discovery Operator and the NVidia GPU Operator using GitOps, you need to confirm that the Nvidia operator is correctly retrieving the GPU information. You can use the following command to confirm that OpenShift is correctly configured:

[source, bash]
---
oc exec -it -n nvidia-gpu-operator $(oc get pod -owide -lopenshift.driver-toolkit=true -o jsonpath="{.items[0].metadata.name}" -n nvidia-gpu-operator) -- nvidia-smi
----

The output should look like this:

[source, bash]
----
Tue Jul  9 15:08:22 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   23C    P8             22W /  300W |       0MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
----

Now, you need to reset RHOAI so that it retrieves the latest GPU configuration:

[source, bash]
---
oc delete cm migration-gpu-status -n redhat-ods-applications; sleep 3; oc delete pods -l app=rhods-dashboard -n redhat-ods-applications
----

Wait for a few seconds until the dashboard pods start again and you will see in the RHOAI web console that now the `NVidia GPU` Accelerator Profile is listed. 


=== Data Connection Pipelines S3 Bucket Secret


The `DataSciencePipelineApplication` requires an S3-compatible storage solution to store artifacts that are generated in the pipeline. You can use any S3-compatible storage solution for data science pipelines, including AWS S3, OpenShift Data Foundation, or MinIO. In this exercise, as our cluster is on AWS, we will use AWS S3.

First, define the configuration variables for AWS is a file dubbed `aws-env-vars`. Then, execute the following command:

[source, bash]
----
./prerequisites/s3-bucket/create-aws-s3-bucket.sh
----









== Installation

[source, bash]
----
oc apply -f application-rhoai.yaml
----












== Useful links

* https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2.8[Official documentation].
* https://access.redhat.com/support/policy/updates/rhoai/service[KCS: Red Hat OpenShift AI Service Definition].
* https://github.com/stefan-bergstein/rhoai-on-rhdh-template/tree/main/manifests/helm/ds-project
* https://github.com/stratus-ss/openshift-ai/blob/main/docs/rendered/OpenShift_AI_CLI.md

* https://issues.redhat.com/projects/RHOAIENG/issues
* https://github.com/mamurak/os-mlops/tree/main/manifests/odh
* https://access.redhat.com/articles/rhoai-supported-configs


* Getting started: https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html-single/getting_started_with_red_hat_openshift_ai_self-managed/index
* Monitoring: https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html-single/serving_models/index#monitoring-model-performance_monitoring-model-performance
* DS Pipelines: https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines




Study sources:
* https://redhatquickcourses.github.io/rhods-admin/rhods-admin/1.33
* https://redhatquickcourses.github.io/rhods-intro/rhods-intro/1.33
* https://redhatquickcourses.github.io/rhods-model/rhods-model/1.33
* https://rh-aiservices-bu.github.io/insurance-claim-processing/modules/02-03-creating-workbench.html
* https://developers.redhat.com/products/red-hat-openshift-ai/getting-started
